{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38567215",
   "metadata": {},
   "source": [
    "Useful links\n",
    "- Unsupervised Text Classification with Topic Models and Good Old Human Reasoning https://medium.com/@power.up1163/unsupervised-text-classification-with-topic-models-and-good-old-human-reasoning-da297bed7362\n",
    "- Understanding Text Classification in NLP with Movie Review Example https://www.analyticsvidhya.com/blog/2020/12/understanding-text-classification-in-nlp-with-movie-review-example-example/\n",
    "- Text Clustering with Unsupervised Learning https://www.kaggle.com/code/carlosaguayo/text-clustering-with-unsupervised-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90f7792",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T14:41:57.182784Z",
     "start_time": "2024-01-30T14:41:53.264817Z"
    }
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "\n",
    "import vertica_python\n",
    "from vertica_python.errors import MissingRelation\n",
    "import verticapy as vp\n",
    "from verticapy import pandas_to_vertica, insert_into, drop\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "import string\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ed0db4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T14:41:58.993838Z",
     "start_time": "2024-01-30T14:41:58.357639Z"
    }
   },
   "outputs": [],
   "source": [
    "config_path = 'config.ini'\n",
    "def get_VRT_cursor(path=config_path):\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(path)\n",
    "    \n",
    "    credentials = {\n",
    "        'database': config['VERTICA']['database'],\n",
    "        'user': config['VERTICA']['user'],\n",
    "        'password': config['VERTICA']['password'],\n",
    "        'host': config['VERTICA']['host'],\n",
    "        'port': config['VERTICA']['port']\n",
    "    }\n",
    "    \n",
    "    conn = vertica_python.connect(**credentials)\n",
    "    cursor = conn.cursor()\n",
    "    return cursor, conn\n",
    "\n",
    "def get_df_from_sql(cursor, sql='SELECT now(), version()'):\n",
    "    cursor.execute(sql)\n",
    "    columns = cursor.description    \n",
    "    result = cursor.fetchall()    \n",
    "    \n",
    "    df = pd.DataFrame(result, columns=[tuple[0] for tuple in columns])\n",
    "    \n",
    "    return df\n",
    "\n",
    "VRT_cursor, conn = get_VRT_cursor()\n",
    "vp.set_connection(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d80b91e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T15:00:13.883262Z",
     "start_time": "2024-01-30T14:50:36.675043Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing period  0\n",
      "Processing period  1\n",
      "Processing period  2\n",
      "Processing period  3\n",
      "Processing period  4\n",
      "Processing period  5\n"
     ]
    }
   ],
   "source": [
    "df = {}\n",
    "\n",
    "for i in range(6):\n",
    "    print(\"Processing period \", i)    \n",
    "    try:\n",
    "        df[i] = get_df_from_sql(cursor=VRT_cursor, \n",
    "                                  sql=f\"\"\"select * from tbl\"\"\")\n",
    "    except Exception as e:\n",
    "        print(\"Unable to process because of exception\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9006ca8",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "048f50d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T15:01:05.818219Z",
     "start_time": "2024-01-30T15:00:14.056428Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(6):\n",
    "    print(\"Processing period\", i)\n",
    "    df[i]['dialog'] = df[i].groupby(['user_id'])['text'].transform(' '.join)\n",
    "    df[i] = df[i][['user_id', 'dialog']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95ad63ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T17:33:32.395625Z",
     "start_time": "2024-01-30T17:29:30.967033Z"
    }
   },
   "outputs": [],
   "source": [
    "def data_preparation(df):\n",
    "    # tokenize the text\n",
    "    df['dialog_processed'] = df['dialog'].map(lambda x: nltk.word_tokenize(x))\n",
    "\n",
    "    # lowercase the tokens\n",
    "    df['dialog_processed'] = df['dialog_processed'].map(lambda x: [token.lower() for token in x])\n",
    "\n",
    "    # get list of stopwords in English\n",
    "    stopwords = nltk.corpus.stopwords.words(\"english\") + ['hi', 'hello', 'hey', 'hii', 'sym', \n",
    "                                                          'f', 'ok', 'yes', 'ye', 'yeah',\n",
    "                                                          'f:1', 'f:2', 'f:3', 'f:4', \n",
    "                                                          'want', 'wanna', 'dont', 'u', 'baby', 'babe',\n",
    "                                                          'love']\n",
    "    # remove stopwords\n",
    "    df['dialog_processed'] = df['dialog_processed'].map(lambda x: [token for token in x \n",
    "                                                                   if token.lower() not in stopwords])\n",
    "\n",
    "    # remove punctuation\n",
    "    df['dialog_processed'] = df['dialog_processed'].map(lambda x: [token for token in x \n",
    "                                                                  if token not in string.punctuation])\n",
    "\n",
    "    # remove numbers\n",
    "    df['dialog_processed'] = df['dialog_processed'].map(lambda x: [token for token in x \n",
    "                                                                  if token not in string.digits])\n",
    "\n",
    "    #remove words with length <= 2 symbols\n",
    "    df['dialog_processed'] = df['dialog_processed'].map(lambda x: [token for token in x if len(token) > 2])\n",
    "\n",
    "    # remove non-informative records\n",
    "    df['dialog_processed_len'] = df['dialog_processed'].map(lambda x: len(x))\n",
    "    df = df[df['dialog_processed_len'] > 0 ]\n",
    "    \n",
    "    # remove smileys\n",
    "    df['dialog_processed'] = df['dialog_processed'].map(lambda x: [token for token in x \n",
    "                                                                   if not(token.startswith('cat_'))])\n",
    "    df['dialog_processed'] = df['dialog_processed'].map(lambda x: [token for token in x \n",
    "                                                                   if not(token.startswith('den_'))])\n",
    "    df['dialog_processed'] = df['dialog_processed'].map(lambda x: [token for token in x \n",
    "                                                                   if not(token.startswith('standard_'))])\n",
    "    df['dialog_processed'] = df['dialog_processed'].map(lambda x: [token for token in x \n",
    "                                                                   if not(token.startswith('htf_'))])\n",
    "    df['dialog_processed'] = df['dialog_processed'].map(lambda x: [token for token in x \n",
    "                                                                   if not(token.startswith('jerboa_'))])\n",
    "    df['dialog_processed'] = df['dialog_processed'].map(lambda x: [token for token in x \n",
    "                                                                   if not(token.startswith('smile_'))])\n",
    "    df['dialog_processed'] = df['dialog_processed'].map(lambda x: [token for token in x \n",
    "                                                                   if not(token.startswith('owl_'))])\n",
    "    df['dialog_processed'] = df['dialog_processed'].map(lambda x: [token for token in x \n",
    "                                                                   if not(token.startswith('ginger_'))])                                                               \n",
    "\n",
    "    # remove non-informative records\n",
    "    df['dialog_processed_len'] = df['dialog_processed'].map(lambda x: len(x))\n",
    "    df = df[df['dialog_processed_len'] > 0 ]\n",
    "    \n",
    "    df_ = df[[\"user_id\", \"dialog\", \"dialog_processed\"]]\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "677654f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T17:47:56.977315Z",
     "start_time": "2024-01-30T17:47:11.401398Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(6):\n",
    "    df[i]  = data_preparation(df[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9843473b",
   "metadata": {},
   "source": [
    "### Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0600d9",
   "metadata": {},
   "source": [
    "Word map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a47c4f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T17:49:42.140115Z",
     "start_time": "2024-01-30T17:47:57.278857Z"
    }
   },
   "outputs": [],
   "source": [
    "df[5]['dialog_processed_string'] = df[5]['dialog_processed'].apply(lambda x: \" \".join(i for i in x))\n",
    "long_string = ' '.join(df['dialog_processed_string'])\n",
    "\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=10000, contour_width=3, contour_color='steelblue')\n",
    "wordcloud.generate(long_string)\n",
    "plt.imshow(wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6223de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_words(model, feature_names, n_top_words, title):\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(30, 15), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[-n_top_words:]\n",
    "        top_features = feature_names[top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 30})\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "        for i in \"top right left\".split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=40)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eabcc7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 10\n",
    "n_top_words = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738398fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from topicwizard.pipeline import make_topic_pipeline\n",
    "\n",
    "long_string_ = [long_string]\n",
    "#tf_vectorizer.fit_transform(long_string_)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_df=2, min_df=0.95, max_features=n_features, stop_words=\"english\"\n",
    ")\n",
    "tfidf = tfidf_vectorizer.fit_transform(long_string_)\n",
    "nmf = NMF(n_components=n_components).fit(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fe754e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "plot_top_words(\n",
    "    nmf, tfidf_feature_names, n_top_words, \"Topics in NMF model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454dc479",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer = CountVectorizer(\n",
    "    max_df=2, min_df=0.95, max_features=n_features, stop_words=\"english\"\n",
    ")\n",
    "tf = tf_vectorizer.fit_transform(long_string_)\n",
    "\n",
    "\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=n_components,\n",
    "    max_iter=10,\n",
    "    learning_method=\"online\",\n",
    "    learning_offset=50.0,\n",
    "    random_state=0,\n",
    ")\n",
    "lda.fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769b27ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_feature_names = tf_vectorizer.get_feature_names_out()\n",
    "plot_top_words(lda, tf_feature_names, 5, \"Topics in LDA model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167be424",
   "metadata": {},
   "source": [
    "### Get features from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b4fe9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_features = ['dog', 'cat', 'bird', 'fish']\n",
    "users_with_features = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1a208d",
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms_features = {}\n",
    "\n",
    "for feature in my_features:\n",
    "    synonyms = []\n",
    "    for syn in wordnet.synsets(feature): \n",
    "        for l in syn.lemmas(): \n",
    "            synonyms.append(l.name().lower().replace('_', ' ')) \n",
    "    synonyms.append(feature.lower().replace('_', ' '))\n",
    "    synonyms_features[feature] = set(synonyms)    \n",
    "\n",
    "synonyms_features['dog'].add('dachshund')  \n",
    "print(synonyms_features) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45094144",
   "metadata": {},
   "source": [
    "Extract users for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5efc6207",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in synonyms_features.keys():\n",
    "    users_with_features[a] = pd.DataFrame(columns=['user_id', 'dialog', 'dialog_processed'])\n",
    "    for i in range(6):\n",
    "        for syn in synonyms_features[a]:\n",
    "            mask = df[i].dialog_processed.apply(lambda x: syn in x)\n",
    "            synonyms_features[a] = pd.concat([synonyms_features[a], df[i][mask]], ignore_index=True)\n",
    "\n",
    "    users_with_features[a] = users_with_features[a].drop_duplicates([\"user_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cc3e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['feature', 'users_count'])\n",
    "\n",
    "for a in my_features:\n",
    "    results.loc[len(results.index)] = [a, users_with_features[a].shape[0]] \n",
    "    \n",
    "results.sort_values(by=['users_count'], ascending=False, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8785f67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "local-venv",
   "language": "python",
   "name": "local-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
